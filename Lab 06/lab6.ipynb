{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labolatorium nr 6 - Search engine\n",
    "\n",
    "#### Patryk Klatka\n",
    "#### 19 kwietnia 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wstęp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Celem tego labolatorium było stworzenie wyszukiwarki dokumentów na podstawie zapytania oraz zbadanie wpływu SVD (dokładniej Low Rank Approximation z wykorzystaniem Singular Value Decomposition, dalej określane skrótowo jako SVD) oraz IDF (Inverse Document Frequency) na wyniki wyszukiwań. Do konstrukcji zbioru dokumentów zostało wykorzystane oficjalne API angielskiej Wikipedii. Zostały pobrane artykuły z kategorii Computed, gdzie ich liczebność wyniosła 7391. Następnie dla każdego artykułu został utworzony bag-of-words, uprzednio wykonując proces lemmatyzacji i usuwając tzw. stop words. Łącznie bag-of-words zawierało 46382 słowa. Następnie zostały wyznaczone macierze z zastosowaniem IDF oraz SVD. Dla SVD (Low Rank Approximation) parametr k został wybrany na podstawie prób w trakcie wykonywania ćwiczenia. Dodatkowo została utworzona aplikacja graficzna w postaci strony internetowej, która pozwala na łatwiejsze wyszukiwanie artykułów.\n",
    "\n",
    "Sprawozdanie zostało napisane w Jupyter Notebooku, w celu przedstawienia nie tylko wniosków z przeprowadzonego labolatorium, ale również kodu, który został wykorzystany do jego wykonania."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import bibliotek oraz ich konfiguracja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import spacy\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from collections import Counter\n",
    "import os\n",
    "import scipy as sp\n",
    "import requests\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "global_folder_name = \"\"  # Used to store data in different folders for different runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pobranie artykułów z danej kategorii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W celu znalezienia artykułów w kategorii została napisana prosta funkcja, która przechodzi rekurencyjnie po podkategoriach danej kategorii i zwraca listę artykułów w niej zawartych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles_from_category(start_category, max_article_number=500):\n",
    "    current_articles = 0\n",
    "    articles = []\n",
    "    stack = [start_category]\n",
    "    visited = set([start_category])\n",
    "\n",
    "    while current_articles < max_article_number and stack:\n",
    "        category = stack.pop()\n",
    "        # Get articles\n",
    "        response = requests.get(\n",
    "            f\"https://en.wikipedia.org/w/api.php?\"+\n",
    "            f\"action=query&list=categorymembers&cmtitle={category}\"+\n",
    "            f\"&cmlimit=500&format=json\")\n",
    "\n",
    "        response = response.json()\n",
    "\n",
    "        # Get query\n",
    "        query = response[\"query\"]['categorymembers']\n",
    "\n",
    "        for article in query:\n",
    "            if article[\"ns\"] == 0:\n",
    "                articles.append(article)\n",
    "                current_articles += 1\n",
    "                if current_articles == max_article_number:\n",
    "                    stack = []\n",
    "                    break\n",
    "            elif article[\"ns\"] == 14 and article[\"title\"] not in visited:\n",
    "                stack.append(article[\"title\"])\n",
    "                visited.add(article[\"title\"])\n",
    "\n",
    "    # Filter duplicates\n",
    "    pageid_set = set()\n",
    "    filtered_articles = []\n",
    "    for article in articles:\n",
    "        if article[\"pageid\"] not in pageid_set:\n",
    "            pageid_set.add(article[\"pageid\"])\n",
    "            filtered_articles.append(article)\n",
    "\n",
    "    start_category = start_category.replace(\":\", \"-\")\n",
    "    with open(f'./pages-from-categories/{start_category}.pickle', 'wb') as f:\n",
    "        pickle.dump(filtered_articles, f)\n",
    "\n",
    "    return filtered_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pobieranie tekstu z artykułu o podanym id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po pobraniu listy artykułów, została napisana funkcja, która pobiera tekst artykułu o podanym id, parsuje go i tworzy bag-of-words odpowiednio przed tym stosując lemmatyzację i usuwanie tzw. stop words. Każdy artykuł został zapisany w osobnym pliku, gdzie nazwa pliku to id artykułu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages(filename, number_of_articles=100, thread_pool_size=10):\n",
    "    if number_of_articles < thread_pool_size:\n",
    "        thread_pool_size = number_of_articles\n",
    "\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    data = list(map(lambda x: x[\"pageid\"], data))\n",
    "\n",
    "    # Get random titles\n",
    "    random_titles = np.random.choice(data, number_of_articles, replace=False)\n",
    "\n",
    "    # Split titles into chunks\n",
    "    random_titles = np.array_split(random_titles, thread_pool_size)\n",
    "\n",
    "    errors = 0\n",
    "\n",
    "    def create_dict(pageids):\n",
    "        errors = 0\n",
    "        for pageid in pageids:\n",
    "            try:\n",
    "                response = requests.get(\n",
    "                    f\"https://en.wikipedia.org/w/api.php?format=json&action=query\"+\n",
    "                    f\"&prop=extracts&explaintext&exsectionformat=wiki\"+\n",
    "                    f\"&redirects=1&pageids={pageid}\")\n",
    "\n",
    "                page = response.json()\n",
    "                article_id = list(page['query']['pages'].keys())[0]\n",
    "                content = re.sub(\n",
    "                    r\"={2} .+\", \"\", page['query']['pages'][article_id]['extract'])\n",
    "\n",
    "                # Tokenize text\n",
    "                tokens = re.findall(\n",
    "                    \"[^\\W\\d_]+\", content)\n",
    "\n",
    "                # Lemmatize tokens and remove stop words\n",
    "                tokens = [token.lemma_.lower() for token in nlp(\n",
    "                    \" \".join(tokens)) if token.lemma_ not in STOP_WORDS]\n",
    "\n",
    "                # Count tokens\n",
    "                counted_tokens = Counter(tokens)\n",
    "                counted_tokens = {k: v for k,\n",
    "                                v in counted_tokens.items() if v > 1}\n",
    "\n",
    "                # Get summary\n",
    "                summary_request = requests.get(\n",
    "                    f\"https://en.wikipedia.org/w/api.php?\"+\n",
    "                    f\"format=json&exintro&action=query&prop=extracts\"+\n",
    "                    f\"&explaintext&exsectionformat=wiki&redirects=1\"+\n",
    "                    f\"&pageids={article_id}&exsentences=2\")\n",
    "\n",
    "                summary = summary_request.json()\n",
    "\n",
    "                result = {\n",
    "                    \"title\": page['query']['pages'][article_id]['title'],\n",
    "                    \"pageid\": article_id,\n",
    "                    \"link\": f\"https://en.wikipedia.org/wiki?curid={article_id}\",\n",
    "                    \"tokens\": counted_tokens,\n",
    "                    \"tokensNumber\": sum(counted_tokens.values()),\n",
    "                    \"summary\": summary[\"query\"][\"pages\"][article_id][\"extract\"],\n",
    "                }\n",
    "\n",
    "                filepath = f\"./parsed-articles{global_folder_name}/article-{article_id}.pickle\"\n",
    "                with open(filepath, \"wb\") as f:\n",
    "                    pickle.dump(result, f)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR: {pageid} - {e}\")\n",
    "                errors += 1\n",
    "        return errors\n",
    "\n",
    "    with ThreadPool(thread_pool_size) as pool:\n",
    "        # Call a function on each item in a list and handle results\n",
    "        for result in pool.map(create_dict, random_titles):\n",
    "            # Count errors\n",
    "            errors += result\n",
    "\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utworzenie macierzy rzadkiej"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Następnie, po pobraniu artykułów, została utworzona funkcja, która tworzy macierz rzadką, gdzie wiersze odpowiadają słowom z bag-of-words, a kolumny artykułom, odpowiednio z możliwością zastosowania IDF lub SVD. W przypadku tej funkcji, parametr k określa ile najmniejszych wartości osobliwych ma zostać usuniętych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sparse_matrix(n=-1, use_idf=False, k=-1):\n",
    "    # Get n articles\n",
    "    i = 0\n",
    "    articles = []\n",
    "    words_in_articles = {}\n",
    "    for file in os.listdir('./parsed-articles'):\n",
    "        if os.path.getsize(f\"./parsed-articles/{file}\") == 0:\n",
    "            continue\n",
    "\n",
    "        with open(f\"./parsed-articles/{file}\", \"rb\") as f:\n",
    "            article = pickle.load(f)\n",
    "            for word in article[\"tokens\"].keys():\n",
    "                if word not in words_in_articles:\n",
    "                    words_in_articles[word] = 0\n",
    "                words_in_articles[word] += 1\n",
    "\n",
    "            articles.append(\n",
    "                {\"title\": article[\"title\"], \n",
    "                 \"pageid\": article[\"pageid\"], \n",
    "                 \"link\": article[\"link\"], \n",
    "                 \"summary\": article[\"summary\"]})\n",
    "            i += 1\n",
    "\n",
    "        if n != -1 and i == n:\n",
    "            break\n",
    "\n",
    "    word_set = list(words_in_articles.keys())\n",
    "\n",
    "    # Map words to indexes\n",
    "    word_set_index = {k: v for v, k in enumerate(word_set)}\n",
    "\n",
    "    # Create sparse matrix\n",
    "    matrix = sp.sparse.lil_matrix((len(articles), len(word_set)))\n",
    "\n",
    "    # Fill sparse matrix\n",
    "    for i, article in enumerate(articles):\n",
    "        filename = f\"./parsed-articles/article-{article['pageid']}.pickle\"\n",
    "        if os.path.getsize(filename) == 0:\n",
    "            continue\n",
    "\n",
    "        with open(filename, \"rb\") as f:\n",
    "            article = pickle.load(f)\n",
    "            for word in article[\"tokens\"]:\n",
    "                matrix[i, word_set_index[word]] = article[\"tokens\"][word]\n",
    "\n",
    "    idf_text = \"\"\n",
    "    svd_text = \"\"\n",
    "\n",
    "    # Normalize matrix using Inverse Document Frequency\n",
    "    if use_idf:\n",
    "        for i in range(matrix.shape[0]):\n",
    "            matrix[i] = matrix[i] * \\\n",
    "                np.log(matrix.shape[0] / words_in_articles[word_set[i]])\n",
    "        idf_text = \"-idf\"\n",
    "\n",
    "    # Remove noises using SVD\n",
    "    if k != -1:\n",
    "        matrix = da.asarray(matrix.transpose())\n",
    "        u, s, vt = da.linalg.svd_compressed(matrix, k=min(matrix.shape)-k)\n",
    "        # u, s, vt = sp.sparse.linalg.svds(matrix, k=k)\n",
    "        matrix = None  # Free memory\n",
    "        matrix = sp.sparse.csr_matrix(u * s @ vt).transpose()\n",
    "        svd_text = \"-svd\"\n",
    "\n",
    "    # Transpose matrix\n",
    "    matrix = matrix.transpose()\n",
    "\n",
    "    # Get better memory representation\n",
    "    matrix = matrix.tocsr()\n",
    "\n",
    "    # Calculate matrix norm\n",
    "    matrix_norm = sp.sparse.linalg.norm(matrix, axis=0)\n",
    "\n",
    "    # Save objects to pickle files\n",
    "    matrix_filename = f\"./calculated-components/matrix{svd_text}{idf_text}.pickle\"\n",
    "    with open(matrix_filename, \"wb\") as f:\n",
    "        pickle.dump((matrix, matrix_norm), f)\n",
    "\n",
    "    with open(\"./calculated-components/word_set.pickle\", \"wb\") as f:\n",
    "        pickle.dump(word_set, f)\n",
    "\n",
    "    with open(\"./calculated-components/articles.pickle\", \"wb\") as f:\n",
    "        pickle.dump(articles, f)\n",
    "\n",
    "    print(\"Number of words:\", len(word_set))\n",
    "    print(\"Number of articles:\", len(articles))\n",
    "\n",
    "    return matrix, word_set, articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utworzenie macierzy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wygenerowano cztery macierze, dla każdej z nich wykorzystano inną metodę tworzenia macierzy. Wszystkie macierze zostały zapisane do plików, aby nie musieć ich ponownie tworzyć. Parametr k został wybrany na podstawie wcześniejszych prób, gdzie zostały wyznaczone wartości k dla różnych wartości SVD i wybrana taka, która dawała najlepsze wyniki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category:Computing\n",
    "category_name = \"Category:Computing\"\n",
    "x = get_articles_from_category(category_name, 10000)\n",
    "\n",
    "category_name = category_name.replace(':', '-')\n",
    "with open(f\"./pages-from-categories/{category_name}.pickle\", 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# global_folder_name = f\"-{category_name}\"\n",
    "global_folder_name = f\"\"\n",
    "os.mkdir(f\"./parsed-articles{global_folder_name}\")\n",
    "res = get_pages(\n",
    "    f'./pages-from-categories/{category_name}.pickle', len(data), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = build_sparse_matrix()\n",
    "articles = build_sparse_matrix(use_idf=True)\n",
    "articles = build_sparse_matrix(k=4200, use_idf=True)\n",
    "articles = build_sparse_matrix(k=4200, use_idf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zapytania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Każde zapytanie w postaci ciągów znaków jest zamieniane na wektor 0/1. Następnie dla każdego artykułu wyznaczana jest podobieństwo między wektorem zapytania a wektorem artykułu (korelacja między wektorami). Wyniki są sortowane malejąco i zwracane zgodnie z parametrami funkcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_from_query_vector(query, k=10, matrix_filename=\"matrix\"):\n",
    "    # Get matrix\n",
    "    with open(f\"./calculated-components/{matrix_filename}.pickle\", \"rb\") as f:\n",
    "        matrix, matrix_norm = pickle.load(f)\n",
    "\n",
    "    # Get word set\n",
    "    with open(\"./calculated-components/word_set.pickle\", \"rb\") as f:\n",
    "        word_set = pickle.load(f)\n",
    "\n",
    "    # Get articles\n",
    "    with open(\"./calculated-components/articles.pickle\", \"rb\") as f:\n",
    "        articles = pickle.load(f)\n",
    "\n",
    "    # Create query vector\n",
    "    query_vector = sp.sparse.lil_matrix((len(word_set), 1))\n",
    "\n",
    "    # Fill query vector\n",
    "    for word in re.findall(\"[^\\W\\d_]+\", query.lower()):\n",
    "        if word in word_set:\n",
    "            query_vector[word_set.index(word)] = 1\n",
    "\n",
    "    # Get vector norm\n",
    "    query_vector_norm = sp.sparse.linalg.norm(query_vector)\n",
    "    norm = query_vector_norm * matrix_norm\n",
    "\n",
    "    # Calculate probabilities\n",
    "    probabilities = (query_vector.T @ matrix) / norm\n",
    "\n",
    "    probabilities = [(probabilities[0, i], i) for i in range(matrix.shape[1])]\n",
    "\n",
    "    # Sort probabilities\n",
    "    probabilities = list(map(lambda x: (x[0], articles[x[1]]), \n",
    "                             filter(lambda x: not np.isnan(x[0]) \n",
    "                                    and np.isfinite(x[0]) \n",
    "                                    and x[0] > 0,\n",
    "                                      sorted(probabilities, key=lambda x: x[0], reverse=True))))\n",
    "\n",
    "    # Get top k results\n",
    "    return probabilities[: k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przykładowe zapytanie nr 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception reporting mode: Plain\n",
      "Matrix results:\n",
      "[(0.7730206825239258, {'title': 'Algorithmic curation', 'pageid': '73233518', 'link': 'https://en.wikipedia.org/wiki?curid=73233518', 'summary': 'Algorithmic curation is the curation (organizing and maintaining a collection) of online media using computer algorithms. Examples include search engine algorithms and social media algorithms.'}), (0.5784924073369803, {'title': 'Jewels of Stringology', 'pageid': '63902887', 'link': 'https://en.wikipedia.org/wiki?curid=63902887', 'summary': 'Jewels of Stringology: Text Algorithms is a book on algorithms for pattern matching in strings and related problems. It was written by Maxime Crochemore and Wojciech Rytter, and published by World Scientific in 2003.'}), (0.5, {'title': 'Algorithms Unlocked', 'pageid': '46573763', 'link': 'https://en.wikipedia.org/wiki?curid=46573763', 'summary': 'Algorithms Unlocked is a book by Thomas H. Cormen about the basic principles and  applications of computer algorithms. The book consists of ten chapters, and deals with the topics of searching, sorting, basic graph algorithms, string processing, the fundamentals of cryptography and data compression, and an introduction to the theory of computation.'})]\n",
      "\n",
      "Matrix-SVD results:\n",
      "[(0.30709433427322774, {'title': 'The Master Algorithm', 'pageid': '47937215', 'link': 'https://en.wikipedia.org/wiki?curid=47937215', 'summary': 'The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World is a book by Pedro Domingos released in 2015. Domingos wrote the book in order to generate interest from people outside the field.'}), (0.29941945881163357, {'title': 'How to Solve it by Computer', 'pageid': '4104986', 'link': 'https://en.wikipedia.org/wiki?curid=4104986', 'summary': 'How to Solve it by Computer is a computer science book by R. G. Dromey, first published by Prentice-Hall in 1982.\\nIt is occasionally used as a textbook, especially in India.It is an introduction to the whys of algorithms and data structures.'}), (0.2756237773223144, {'title': '9 Algorithms That Changed the Future', 'pageid': '46936585', 'link': 'https://en.wikipedia.org/wiki?curid=46936585', 'summary': '9 Algorithms that Changed the Future is a 2012 book by John MacCormick on algorithms. The book seeks to explain commonly encountered computer algorithms to a layman audience.'})]\n",
      "\n",
      "Matrix-IDF results:\n",
      "[(0.7730206825239259, {'title': 'Algorithmic curation', 'pageid': '73233518', 'link': 'https://en.wikipedia.org/wiki?curid=73233518', 'summary': 'Algorithmic curation is the curation (organizing and maintaining a collection) of online media using computer algorithms. Examples include search engine algorithms and social media algorithms.'}), (0.57849240733698, {'title': 'Jewels of Stringology', 'pageid': '63902887', 'link': 'https://en.wikipedia.org/wiki?curid=63902887', 'summary': 'Jewels of Stringology: Text Algorithms is a book on algorithms for pattern matching in strings and related problems. It was written by Maxime Crochemore and Wojciech Rytter, and published by World Scientific in 2003.'}), (0.5, {'title': 'Algorithms Unlocked', 'pageid': '46573763', 'link': 'https://en.wikipedia.org/wiki?curid=46573763', 'summary': 'Algorithms Unlocked is a book by Thomas H. Cormen about the basic principles and  applications of computer algorithms. The book consists of ten chapters, and deals with the topics of searching, sorting, basic graph algorithms, string processing, the fundamentals of cryptography and data compression, and an introduction to the theory of computation.'})]\n",
      "\n",
      "Matrix-SVD-IDF results:\n",
      "[(0.32593771325315674, {'title': 'How to Solve it by Computer', 'pageid': '4104986', 'link': 'https://en.wikipedia.org/wiki?curid=4104986', 'summary': 'How to Solve it by Computer is a computer science book by R. G. Dromey, first published by Prentice-Hall in 1982.\\nIt is occasionally used as a textbook, especially in India.It is an introduction to the whys of algorithms and data structures.'}), (0.3068792831677077, {'title': 'The Master Algorithm', 'pageid': '47937215', 'link': 'https://en.wikipedia.org/wiki?curid=47937215', 'summary': 'The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World is a book by Pedro Domingos released in 2015. Domingos wrote the book in order to generate interest from people outside the field.'}), (0.297439081749609, {'title': '9 Algorithms That Changed the Future', 'pageid': '46936585', 'link': 'https://en.wikipedia.org/wiki?curid=46936585', 'summary': '9 Algorithms that Changed the Future is a 2012 book by John MacCormick on algorithms. The book seeks to explain commonly encountered computer algorithms to a layman audience.'})]\n"
     ]
    }
   ],
   "source": [
    "search_string = \"algorithm\"\n",
    "print(\"Matrix results:\")\n",
    "print(get_results_from_query_vector(search_string, k=3, matrix_filename=\"matrix\"))\n",
    "print(\"\\nMatrix-SVD results:\")\n",
    "print(get_results_from_query_vector(search_string, k=3, matrix_filename=\"matrix-svd\"))\n",
    "print(\"\\nMatrix-IDF results:\")\n",
    "print(get_results_from_query_vector(search_string, k=3, matrix_filename=\"matrix-idf\"))\n",
    "print(\"\\nMatrix-SVD-IDF results:\")\n",
    "print(get_results_from_query_vector(search_string, k=3, matrix_filename=\"matrix-svd-idf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przykładowe zapytanie nr 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix results:\n",
      "[(0.5547001962252291, {'title': 'FCX file compression', 'pageid': '17974308', 'link': 'https://en.wikipedia.org/wiki?curid=17974308', 'summary': 'FCX file compression is a file compression utility and file format.  It is supported on a large number of platforms.'}), (0.5321520841901914, {'title': 'Data compression symmetry', 'pageid': '8590317', 'link': 'https://en.wikipedia.org/wiki?curid=8590317', 'summary': 'Symmetry and asymmetry, in the context of data compression, refer to the time relation between compression and decompression for a given compression algorithm.\\nIf an algorithm takes the same time to compress a data archive as it does to decompress it, it is considered symmetrical.'}), (0.48507125007266594, {'title': 'AZ64', 'pageid': '62075617', 'link': 'https://en.wikipedia.org/wiki?curid=62075617', 'summary': \"AZ64 or AZ64 Encoding is a data compression algorithm proprietary to Amazon Web Services.Amazon claims better compression and better speed than raw, LZO or Zstandard, when used in Amazon's Redshift service.\"})]\n",
      "\n",
      "Matrix-SVD results:\n",
      "[(0.1423192660339416, {'title': 'FCX file compression', 'pageid': '17974308', 'link': 'https://en.wikipedia.org/wiki?curid=17974308', 'summary': 'FCX file compression is a file compression utility and file format.  It is supported on a large number of platforms.'}), (0.10771694849052886, {'title': 'Pack200', 'pageid': '9036753', 'link': 'https://en.wikipedia.org/wiki?curid=9036753', 'summary': 'Pack200, specified in JSR 200 (J2SE 1.5), deprecated in JEP 336 (Java SE 11) and removed in JEP 367 (Java SE 14), is a compacting archive format developed by Sun, capable of reducing JAR file sizes by up to a factor of 9, with a factor of 3 to 4 seen in practice. Pack200 is optimized for compressing JAR archive files, specifically the Java bytecode portion of the JAR files.'}), (0.07752287536680708, {'title': 'Modified Huffman coding', 'pageid': '2458647', 'link': 'https://en.wikipedia.org/wiki?curid=2458647', 'summary': 'Modified Huffman coding is used in fax machines to encode black-on-white images (bitmaps). It combines the variable-length codes of Huffman coding with the coding of repetitive data in run-length encoding.'})]\n",
      "\n",
      "Matrix-IDF results:\n",
      "[(0.5547001962252291, {'title': 'FCX file compression', 'pageid': '17974308', 'link': 'https://en.wikipedia.org/wiki?curid=17974308', 'summary': 'FCX file compression is a file compression utility and file format.  It is supported on a large number of platforms.'}), (0.5321520841901914, {'title': 'Data compression symmetry', 'pageid': '8590317', 'link': 'https://en.wikipedia.org/wiki?curid=8590317', 'summary': 'Symmetry and asymmetry, in the context of data compression, refer to the time relation between compression and decompression for a given compression algorithm.\\nIf an algorithm takes the same time to compress a data archive as it does to decompress it, it is considered symmetrical.'}), (0.48507125007266594, {'title': 'AZ64', 'pageid': '62075617', 'link': 'https://en.wikipedia.org/wiki?curid=62075617', 'summary': \"AZ64 or AZ64 Encoding is a data compression algorithm proprietary to Amazon Web Services.Amazon claims better compression and better speed than raw, LZO or Zstandard, when used in Amazon's Redshift service.\"})]\n",
      "\n",
      "Matrix-SVD-IDF results:\n",
      "[(0.14328660677317248, {'title': 'Pack200', 'pageid': '9036753', 'link': 'https://en.wikipedia.org/wiki?curid=9036753', 'summary': 'Pack200, specified in JSR 200 (J2SE 1.5), deprecated in JEP 336 (Java SE 11) and removed in JEP 367 (Java SE 14), is a compacting archive format developed by Sun, capable of reducing JAR file sizes by up to a factor of 9, with a factor of 3 to 4 seen in practice. Pack200 is optimized for compressing JAR archive files, specifically the Java bytecode portion of the JAR files.'}), (0.07740571309143585, {'title': 'Modified Huffman coding', 'pageid': '2458647', 'link': 'https://en.wikipedia.org/wiki?curid=2458647', 'summary': 'Modified Huffman coding is used in fax machines to encode black-on-white images (bitmaps). It combines the variable-length codes of Huffman coding with the coding of repetitive data in run-length encoding.'}), (0.07634238251186501, {'title': 'Cloop', 'pageid': '1120214', 'link': 'https://en.wikipedia.org/wiki?curid=1120214', 'summary': 'The compressed loop device (cloop) is a module for the Linux kernel.  It adds support for transparently decompressed, read-only block devices.'})]\n"
     ]
    }
   ],
   "source": [
    "search_string = \"compression\"\n",
    "print(\"Matrix results:\")\n",
    "print(get_results_from_query_vector(search_string, k=3, matrix_filename=\"matrix\"))\n",
    "print(\"\\nMatrix-SVD results:\")\n",
    "print(get_results_from_query_vector(search_string, k=3, matrix_filename=\"matrix-svd\"))\n",
    "print(\"\\nMatrix-IDF results:\")\n",
    "print(get_results_from_query_vector(search_string, k=3, matrix_filename=\"matrix-idf\"))\n",
    "print(\"\\nMatrix-SVD-IDF results:\")\n",
    "print(get_results_from_query_vector(search_string, k=3, matrix_filename=\"matrix-svd-idf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przykładowe zapytanie nr 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix results:\n",
      "[(0.6324555320336759, {'title': 'Layered queueing network', 'pageid': '35685456', 'link': 'https://en.wikipedia.org/wiki?curid=35685456', 'summary': 'In queueing theory, a discipline within the mathematical theory of probability, a layered queueing network (or rendezvous network) is a queueing network model where the service time for each job at each service node is given by the response time of a queueing network (and those service times in turn may also be determined by further nested networks). Resources can be nested and queues form along the nodes of the nesting structure.'}), (0.6047078979069521, {'title': 'Command queue', 'pageid': '5187054', 'link': 'https://en.wikipedia.org/wiki?curid=5187054', 'summary': 'In computer science, a command queue is a queue for enabling the delay of command execution, either in order of priority, on a first-in first-out basis, or in any order that serves the current purpose. Instead of waiting for each command to be executed before sending the next one, the program just puts all the commands in the queue and goes on doing other things while the queue is processed by the operating system.'}), (0.565752381856018, {'title': 'FSCAN', 'pageid': '6334799', 'link': 'https://en.wikipedia.org/wiki?curid=6334799', 'summary': \"FSCAN is a disk scheduling algorithm to determine the motion of the disk's arm and head in servicing read and write requests.\\nIt uses two sub-queues.\"})]\n",
      "\n",
      "Matrix-SVD results:\n",
      "[(0.4165806739136975, {'title': 'FSCAN', 'pageid': '6334799', 'link': 'https://en.wikipedia.org/wiki?curid=6334799', 'summary': \"FSCAN is a disk scheduling algorithm to determine the motion of the disk's arm and head in servicing read and write requests.\\nIt uses two sub-queues.\"}), (0.4112755686219322, {'title': 'Command queue', 'pageid': '5187054', 'link': 'https://en.wikipedia.org/wiki?curid=5187054', 'summary': 'In computer science, a command queue is a queue for enabling the delay of command execution, either in order of priority, on a first-in first-out basis, or in any order that serves the current purpose. Instead of waiting for each command to be executed before sending the next one, the program just puts all the commands in the queue and goes on doing other things while the queue is processed by the operating system.'}), (0.38121498316149494, {'title': 'Layered queueing network', 'pageid': '35685456', 'link': 'https://en.wikipedia.org/wiki?curid=35685456', 'summary': 'In queueing theory, a discipline within the mathematical theory of probability, a layered queueing network (or rendezvous network) is a queueing network model where the service time for each job at each service node is given by the response time of a queueing network (and those service times in turn may also be determined by further nested networks). Resources can be nested and queues form along the nodes of the nesting structure.'})]\n",
      "\n",
      "Matrix-IDF results:\n",
      "[(0.6324555320336759, {'title': 'Layered queueing network', 'pageid': '35685456', 'link': 'https://en.wikipedia.org/wiki?curid=35685456', 'summary': 'In queueing theory, a discipline within the mathematical theory of probability, a layered queueing network (or rendezvous network) is a queueing network model where the service time for each job at each service node is given by the response time of a queueing network (and those service times in turn may also be determined by further nested networks). Resources can be nested and queues form along the nodes of the nesting structure.'}), (0.6047078979069521, {'title': 'Command queue', 'pageid': '5187054', 'link': 'https://en.wikipedia.org/wiki?curid=5187054', 'summary': 'In computer science, a command queue is a queue for enabling the delay of command execution, either in order of priority, on a first-in first-out basis, or in any order that serves the current purpose. Instead of waiting for each command to be executed before sending the next one, the program just puts all the commands in the queue and goes on doing other things while the queue is processed by the operating system.'}), (0.5657523818560181, {'title': 'FSCAN', 'pageid': '6334799', 'link': 'https://en.wikipedia.org/wiki?curid=6334799', 'summary': \"FSCAN is a disk scheduling algorithm to determine the motion of the disk's arm and head in servicing read and write requests.\\nIt uses two sub-queues.\"})]\n",
      "\n",
      "Matrix-SVD-IDF results:\n",
      "[(0.4281786304387956, {'title': 'Command queue', 'pageid': '5187054', 'link': 'https://en.wikipedia.org/wiki?curid=5187054', 'summary': 'In computer science, a command queue is a queue for enabling the delay of command execution, either in order of priority, on a first-in first-out basis, or in any order that serves the current purpose. Instead of waiting for each command to be executed before sending the next one, the program just puts all the commands in the queue and goes on doing other things while the queue is processed by the operating system.'}), (0.4278796554225075, {'title': 'FSCAN', 'pageid': '6334799', 'link': 'https://en.wikipedia.org/wiki?curid=6334799', 'summary': \"FSCAN is a disk scheduling algorithm to determine the motion of the disk's arm and head in servicing read and write requests.\\nIt uses two sub-queues.\"}), (0.38135086670716223, {'title': 'Layered queueing network', 'pageid': '35685456', 'link': 'https://en.wikipedia.org/wiki?curid=35685456', 'summary': 'In queueing theory, a discipline within the mathematical theory of probability, a layered queueing network (or rendezvous network) is a queueing network model where the service time for each job at each service node is given by the response time of a queueing network (and those service times in turn may also be determined by further nested networks). Resources can be nested and queues form along the nodes of the nesting structure.'})]\n"
     ]
    }
   ],
   "source": [
    "search_string = \"queue\"\n",
    "print(\"Matrix results:\")\n",
    "print(get_results_from_query_vector(search_string, k=3, matrix_filename=\"matrix\"))\n",
    "print(\"\\nMatrix-SVD results:\")\n",
    "print(get_results_from_query_vector(search_string, k=3, matrix_filename=\"matrix-svd\"))\n",
    "print(\"\\nMatrix-IDF results:\")\n",
    "print(get_results_from_query_vector(search_string, k=3, matrix_filename=\"matrix-idf\"))\n",
    "print(\"\\nMatrix-SVD-IDF results:\")\n",
    "print(get_results_from_query_vector(search_string, k=3, matrix_filename=\"matrix-svd-idf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wnioski"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W zależności od zapytania, wykorzystanie IDF lub SVD wpływa na wyniki. Najlepiej widać to dla zapytania nr 1 dla macierzy IDF-SVD oraz SVD, gdzie zwracane są różne wyniki. Można zauważyć, że dla macierzy z SVD podobieństwa są mniejsze niż dla macierzy bez SVD. Dla macierzy zwykłej oraz IDF nie zauważono większych różnic w wynikach zapytania. W przypadku macierzy z wykorzystaniem SVD, zapytania zazwyczaj wykonywały się wolniej, macierze te również zabierały więcej miejsca na dysku niż macierze bez SVD. Moim zdaniem, najlepszą macierzą do zapytań będzie macierz IDF bez SVD, ponieważ zwraca dobre wyniki oraz zajmuje mniej miejsca na dysku, pomimo że subiektywnie macierz z SVD zwraca lepsze wyniki."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
