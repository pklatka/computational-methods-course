{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labolatorium nr 6 - Search engine\n",
    "\n",
    "#### Patryk Klatka\n",
    "#### 19 kwietnia 2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wstęp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Celem tego labolatorium było stworzenie wyszukiwarki dokumentów oraz zbadanie wpływu SVD na wyniki wyszukiwań. Do konstrukcji zbioru dokumentów zostało wykorzystane oficjalne API Wikipedii. Zostały pobrane artykuły z kategorii Computed, gdzie ich liczebność wyniosła 7391. Następnie dla każdego artykułu został utworzony bag-of-words, uprzednio wykonując proces lemmatyzacji i usuwając tzw. stop words. Łącznie bag-of-words zawierało 46382 słowa. Następnie zostały wyznaczone macierze z zastosowaniem IDF oraz SVD. Dla SVD parametr k został wybrany na podstawie prób w trakcie wykonywania ćwiczenia.\n",
    "\n",
    "Sprawozdanie zostało napisane w Jupyter Notebooku, w celu przedstawienia nie tylko wniosków z przeprowadzonego labolatorium, ale również kodu, który został wykorzystany do jego wykonania."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import bibliotek oraz ich konfiguracja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import spacy\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from collections import Counter\n",
    "import os\n",
    "import scipy as sp\n",
    "import requests\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "global_folder_name = \"\"  # Used to store data in different folders for different runs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pobranie artykułów z danej kategorii"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W celu znalezienia artykułów w kategorii została napisana prosta funkcja, która przechodzi rekurencyjnie po podkategoriach danej kategorii i zwraca listę artykułów w niej zawartych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles_from_category(start_category, max_article_number=500):\n",
    "    current_articles = 0\n",
    "    articles = []\n",
    "    stack = [start_category]\n",
    "    visited = set([start_category])\n",
    "\n",
    "    while current_articles < max_article_number and stack:\n",
    "        category = stack.pop()\n",
    "        # Get articles\n",
    "        response = requests.get(\n",
    "            f\"https://en.wikipedia.org/w/api.php?action=query&list=categorymembers&cmtitle={category}&cmlimit=500&format=json\")\n",
    "\n",
    "        response = response.json()\n",
    "\n",
    "        # Get query\n",
    "        query = response[\"query\"]['categorymembers']\n",
    "\n",
    "        for article in query:\n",
    "            if article[\"ns\"] == 0:\n",
    "                articles.append(article)\n",
    "                current_articles += 1\n",
    "                if current_articles == max_article_number:\n",
    "                    stack = []\n",
    "                    break\n",
    "            elif article[\"ns\"] == 14 and article[\"title\"] not in visited:\n",
    "                stack.append(article[\"title\"])\n",
    "                visited.add(article[\"title\"])\n",
    "\n",
    "    # Filter duplicates\n",
    "    pageid_set = set()\n",
    "    filtered_articles = []\n",
    "    for article in articles:\n",
    "        if article[\"pageid\"] not in pageid_set:\n",
    "            pageid_set.add(article[\"pageid\"])\n",
    "            filtered_articles.append(article)\n",
    "\n",
    "    start_category = start_category.replace(\":\", \"-\")\n",
    "    with open(f'./pages-from-categories/{start_category}.pickle', 'wb') as f:\n",
    "        pickle.dump(filtered_articles, f)\n",
    "\n",
    "    return filtered_articles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pobieranie tekstu z artykułu o podanym id"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po pobraniu listy artykułów, została napisana funkcja, która pobiera tekst artykułu o podanym id, parsuje go i tworzy bag-of-words odpowiednio przed tym stosując lemmatyzację i usuwanie tzw. stop words. Każdy artykuł został zapisany w osobnym pliku, gdzie nazwa pliku to id artykułu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages(filename, number_of_articles=100, thread_pool_size=10):\n",
    "    if number_of_articles < thread_pool_size:\n",
    "        thread_pool_size = number_of_articles\n",
    "\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    data = list(map(lambda x: x[\"pageid\"], data))\n",
    "\n",
    "    # Get random titles\n",
    "    random_titles = np.random.choice(data, number_of_articles, replace=False)\n",
    "\n",
    "    # Split titles into chunks\n",
    "    random_titles = np.array_split(random_titles, thread_pool_size)\n",
    "\n",
    "    errors = 0\n",
    "\n",
    "    def create_dict(pageids):\n",
    "        errors = 0\n",
    "        for pageid in pageids:\n",
    "            try:\n",
    "                response = requests.get(\n",
    "                    f\"https://en.wikipedia.org/w/api.php?format=json&action=query&prop=extracts&explaintext&exsectionformat=wiki&redirects=1&pageids={pageid}\")\n",
    "\n",
    "                page = response.json()\n",
    "                article_id = list(page['query']['pages'].keys())[0]\n",
    "                content = re.sub(\n",
    "                    r\"={2} .+\", \"\", page['query']['pages'][article_id]['extract'])\n",
    "\n",
    "                # Tokenize text\n",
    "                tokens = re.findall(\n",
    "                    \"[^\\W\\d_]+\", content)\n",
    "\n",
    "                # Lemmatize tokens and remove stop words\n",
    "                tokens = [token.lemma_.lower() for token in nlp(\n",
    "                    \" \".join(tokens)) if token.lemma_ not in STOP_WORDS]\n",
    "\n",
    "                # Count tokens\n",
    "                counted_tokens = Counter(tokens)\n",
    "                counted_tokens = {k: v for k,\n",
    "                                v in counted_tokens.items() if v > 1}\n",
    "\n",
    "                # Get summary\n",
    "                summary_request = requests.get(\n",
    "                    f\"https://en.wikipedia.org/w/api.php?format=json&exintro&action=query&prop=extracts&explaintext&exsectionformat=wiki&redirects=1&pageids={article_id}&exsentences=2\")\n",
    "\n",
    "                summary = summary_request.json()\n",
    "\n",
    "                result = {\n",
    "                    \"title\": page['query']['pages'][article_id]['title'],\n",
    "                    \"pageid\": article_id,\n",
    "                    \"link\": f\"https://en.wikipedia.org/wiki?curid={article_id}\",\n",
    "                    \"tokens\": counted_tokens,\n",
    "                    \"tokensNumber\": sum(counted_tokens.values()),\n",
    "                    \"summary\": summary[\"query\"][\"pages\"][article_id][\"extract\"],\n",
    "                }\n",
    "\n",
    "                with open(f\"./parsed-articles{global_folder_name}/article-{article_id}.pickle\", \"wb\") as f:\n",
    "                    pickle.dump(result, f)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR: {pageid} - {e}\")\n",
    "                errors += 1\n",
    "        return errors\n",
    "\n",
    "    with ThreadPool(thread_pool_size) as pool:\n",
    "        # Call a function on each item in a list and handle results\n",
    "        for result in pool.map(create_dict, random_titles):\n",
    "            # Count errors\n",
    "            errors += result\n",
    "\n",
    "    return errors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utworzenie macierzy rzadkiej"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Następnie, po pobraniu artykułów, została utworzona funkcja, która tworzy macierz rzadką, gdzie wiersze odpowiadają słowom z bag-of-words, a kolumny artykułom, odpowiednio z możliwością zastosowania IDF lub SVD. W przypadku tej funkcji, parametr k określa ile najmniejszych wartości osobliwych ma zostać usuniętych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sparse_matrix(n=-1, use_idf=False, k=-1):\n",
    "    # Get n articles\n",
    "    i = 0\n",
    "    articles = []\n",
    "    words_in_articles = {}\n",
    "    for file in os.listdir('./parsed-articles'):\n",
    "        if os.path.getsize(f\"./parsed-articles/{file}\") == 0:\n",
    "            continue\n",
    "\n",
    "        with open(f\"./parsed-articles/{file}\", \"rb\") as f:\n",
    "            article = pickle.load(f)\n",
    "            for word in article[\"tokens\"].keys():\n",
    "                if word not in words_in_articles:\n",
    "                    words_in_articles[word] = 0\n",
    "                words_in_articles[word] += 1\n",
    "\n",
    "            articles.append(\n",
    "                {\"title\": article[\"title\"], \"pageid\": article[\"pageid\"], \"link\": article[\"link\"], \"summary\": article[\"summary\"]})\n",
    "            i += 1\n",
    "\n",
    "        if n != -1 and i == n:\n",
    "            break\n",
    "\n",
    "    word_set = list(words_in_articles.keys())\n",
    "\n",
    "    # Map words to indexes\n",
    "    word_set_index = {k: v for v, k in enumerate(word_set)}\n",
    "\n",
    "    # Create sparse matrix\n",
    "    matrix = sp.sparse.lil_matrix((len(articles), len(word_set)))\n",
    "\n",
    "    # Fill sparse matrix\n",
    "    for i, article in enumerate(articles):\n",
    "        if os.path.getsize(f\"./parsed-articles/article-{article['pageid']}.pickle\") == 0:\n",
    "            continue\n",
    "\n",
    "        with open(f\"./parsed-articles/article-{article['pageid']}.pickle\", \"rb\") as f:\n",
    "            article = pickle.load(f)\n",
    "            for word in article[\"tokens\"]:\n",
    "                matrix[i, word_set_index[word]] = article[\"tokens\"][word]\n",
    "\n",
    "    idf_text = \"\"\n",
    "    svd_text = \"\"\n",
    "\n",
    "    # Normalize matrix using Inverse Document Frequency\n",
    "    if use_idf:\n",
    "        for i in range(matrix.shape[0]):\n",
    "            matrix[i] = matrix[i] * \\\n",
    "                np.log(matrix.shape[0] / words_in_articles[word_set[i]])\n",
    "        idf_text = \"-idf\"\n",
    "\n",
    "    # Remove noises using SVD\n",
    "    if k != -1:\n",
    "        matrix = da.asarray(matrix.transpose())\n",
    "        u, s, vt = da.linalg.svd_compressed(matrix, k=min(matrix.shape)-k)\n",
    "        # u, s, vt = sp.sparse.linalg.svds(matrix, k=k)\n",
    "        matrix = None  # Free memory\n",
    "        matrix = sp.sparse.csr_matrix(u * s @ vt).transpose()\n",
    "        svd_text = \"-svd\"\n",
    "\n",
    "    # Transpose matrix\n",
    "    matrix = matrix.transpose()\n",
    "\n",
    "    # Get better memory representation\n",
    "    matrix = matrix.tocsr()\n",
    "\n",
    "    # Calculate matrix norm\n",
    "    matrix_norm = sp.sparse.linalg.norm(matrix, axis=0)\n",
    "\n",
    "    # Save objects to pickle files\n",
    "    with open(f\"./calculated-components/matrix{svd_text}{idf_text}.pickle\", \"wb\") as f:\n",
    "        pickle.dump((matrix, matrix_norm), f)\n",
    "\n",
    "    with open(\"./calculated-components/word_set.pickle\", \"wb\") as f:\n",
    "        pickle.dump(word_set, f)\n",
    "\n",
    "    with open(\"./calculated-components/articles.pickle\", \"wb\") as f:\n",
    "        pickle.dump(articles, f)\n",
    "\n",
    "    print(\"Number of words:\", len(word_set))\n",
    "    print(\"Number of articles:\", len(articles))\n",
    "\n",
    "    return matrix, word_set, articles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utworzenie macierzy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wygenerowano cztery macierze, dla każdej z nich wykorzystano inną metodę tworzenia macierzy. Wszystkie macierze zostały zapisane do plików, aby nie musieć ich ponownie tworzyć. Parametr k został wybrany na podstawie wcześniejszych prób, gdzie zostały wyznaczone wartości k dla różnych wartości SVD i wybrana taka, która dawała najlepsze wyniki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Category:Computing\n",
    "# category_name = \"Category:Computing\"\n",
    "# x = get_articles_from_category(category_name, 10000)\n",
    "\n",
    "# category_name = category_name.replace(':', '-')\n",
    "# with open(f\"./pages-from-categories/{category_name}.pickle\", 'rb') as f:\n",
    "#     data = pickle.load(f)\n",
    "\n",
    "# # global_folder_name = f\"-{category_name}\"\n",
    "# global_folder_name = f\"\"\n",
    "# os.mkdir(f\"./parsed-articles{global_folder_name}\")\n",
    "# res = get_pages(\n",
    "#     f'./pages-from-categories/{category_name}.pickle', len(data), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# articles = build_sparse_matrix()\n",
    "# articles = build_sparse_matrix(use_idf=True)\n",
    "# articles = build_sparse_matrix(k=3500, use_idf=True)\n",
    "# articles = build_sparse_matrix(k=3500, use_idf=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zapytania"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Każde zapytanie w postaci ciągów znaków jest zamieniane na wektor 0/1. Następnie dla każdego artykułu wyznaczana jest podobieństwo między wektorem zapytania a wektorem artykułu. Wyniki są sortowane malejąco i zwracane zgodnie z parametrami funkcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_from_query_vector(query, k=10, matrix_filename=\"matrix\"):\n",
    "    # Get matrix\n",
    "    with open(f\"./calculated-components/{matrix_filename}.pickle\", \"rb\") as f:\n",
    "        matrix, matrix_norm = pickle.load(f)\n",
    "\n",
    "    # Get word set\n",
    "    with open(\"./calculated-components/word_set.pickle\", \"rb\") as f:\n",
    "        word_set = pickle.load(f)\n",
    "\n",
    "    # Get articles\n",
    "    with open(\"./calculated-components/articles.pickle\", \"rb\") as f:\n",
    "        articles = pickle.load(f)\n",
    "\n",
    "    # Create query vector\n",
    "    query_vector = sp.sparse.lil_matrix((len(word_set), 1))\n",
    "\n",
    "    # Fill query vector\n",
    "    for word in re.findall(\"[^\\W\\d_]+\", query.lower()):\n",
    "        if word in word_set:\n",
    "            query_vector[word_set.index(word)] = 1\n",
    "\n",
    "    # Get vector norm\n",
    "    query_vector_norm = sp.sparse.linalg.norm(query_vector)\n",
    "    norm = query_vector_norm * matrix_norm\n",
    "\n",
    "    # Calculate probabilities\n",
    "    probabilities = (query_vector.T @ matrix) / norm\n",
    "\n",
    "    probabilities = [(probabilities[0, i], i) for i in range(matrix.shape[1])]\n",
    "\n",
    "    # Sort probabilities\n",
    "    probabilities = list(map(lambda x: (x[0], articles[x[1]]), filter(lambda x: not np.isnan(x[0]) and np.isfinite(\n",
    "        x[0]) and x[0] > 0, sorted(probabilities, key=lambda x: x[0], reverse=True))))\n",
    "\n",
    "    # Get top k results\n",
    "    return probabilities[: k]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przykładowe zapytanie nr 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix results:\n",
      "[(0.7730206825239258, {'title': 'Algorithmic curation', 'pageid': '73233518', 'link': 'https://en.wikipedia.org/wiki?curid=73233518', 'summary': 'Algorithmic curation is the curation (organizing and maintaining a collection) of online media using computer algorithms. Examples include search engine algorithms and social media algorithms.'}), (0.5784924073369803, {'title': 'Jewels of Stringology', 'pageid': '63902887', 'link': 'https://en.wikipedia.org/wiki?curid=63902887', 'summary': 'Jewels of Stringology: Text Algorithms is a book on algorithms for pattern matching in strings and related problems. It was written by Maxime Crochemore and Wojciech Rytter, and published by World Scientific in 2003.'}), (0.5, {'title': 'Algorithms Unlocked', 'pageid': '46573763', 'link': 'https://en.wikipedia.org/wiki?curid=46573763', 'summary': 'Algorithms Unlocked is a book by Thomas H. Cormen about the basic principles and  applications of computer algorithms. The book consists of ten chapters, and deals with the topics of searching, sorting, basic graph algorithms, string processing, the fundamentals of cryptography and data compression, and an introduction to the theory of computation.'})]\n",
      "Matrix-SVD results:\n",
      "Matrix-IDF results:\n",
      "[(0.7730206825239259, {'title': 'Algorithmic curation', 'pageid': '73233518', 'link': 'https://en.wikipedia.org/wiki?curid=73233518', 'summary': 'Algorithmic curation is the curation (organizing and maintaining a collection) of online media using computer algorithms. Examples include search engine algorithms and social media algorithms.'}), (0.57849240733698, {'title': 'Jewels of Stringology', 'pageid': '63902887', 'link': 'https://en.wikipedia.org/wiki?curid=63902887', 'summary': 'Jewels of Stringology: Text Algorithms is a book on algorithms for pattern matching in strings and related problems. It was written by Maxime Crochemore and Wojciech Rytter, and published by World Scientific in 2003.'}), (0.5, {'title': 'Algorithms Unlocked', 'pageid': '46573763', 'link': 'https://en.wikipedia.org/wiki?curid=46573763', 'summary': 'Algorithms Unlocked is a book by Thomas H. Cormen about the basic principles and  applications of computer algorithms. The book consists of ten chapters, and deals with the topics of searching, sorting, basic graph algorithms, string processing, the fundamentals of cryptography and data compression, and an introduction to the theory of computation.'})]\n",
      "Matrix-SVD-IDF results:\n",
      "[(0.32593771325315674, {'title': 'How to Solve it by Computer', 'pageid': '4104986', 'link': 'https://en.wikipedia.org/wiki?curid=4104986', 'summary': 'How to Solve it by Computer is a computer science book by R. G. Dromey, first published by Prentice-Hall in 1982.\\nIt is occasionally used as a textbook, especially in India.It is an introduction to the whys of algorithms and data structures.'}), (0.3068792831677077, {'title': 'The Master Algorithm', 'pageid': '47937215', 'link': 'https://en.wikipedia.org/wiki?curid=47937215', 'summary': 'The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World is a book by Pedro Domingos released in 2015. Domingos wrote the book in order to generate interest from people outside the field.'}), (0.297439081749609, {'title': '9 Algorithms That Changed the Future', 'pageid': '46936585', 'link': 'https://en.wikipedia.org/wiki?curid=46936585', 'summary': '9 Algorithms that Changed the Future is a 2012 book by John MacCormick on algorithms. The book seeks to explain commonly encountered computer algorithms to a layman audience.'})]\n"
     ]
    }
   ],
   "source": [
    "search_string = \"algorithm\"\n",
    "print(\"Matrix results:\")\n",
    "print(get_results_from_query_vector(search_string, k=3, matrix_filename=\"matrix\"))\n",
    "print(\"Matrix-SVD results:\")\n",
    "print(get_results_from_query_vector(search_string, k=3, matrix_filename=\"matrix-svd\"))\n",
    "print(\"Matrix-IDF results:\")\n",
    "print(get_results_from_query_vector(search_string, k=3, matrix_filename=\"matrix-idf\"))\n",
    "print(\"Matrix-SVD-IDF results:\")\n",
    "print(get_results_from_query_vector(search_string, k=3, matrix_filename=\"matrix-svd-idf\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przykładowe zapytanie nr 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix results:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m search_string \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcompression\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMatrix results:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[39mprint\u001b[39m(get_results_from_query_vector(search_string, k\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, matrix_filename\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmatrix\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMatrix-SVD results:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(get_results_from_query_vector(search_string, k\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, matrix_filename\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmatrix-svd\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m, in \u001b[0;36mget_results_from_query_vector\u001b[0;34m(query, k, matrix_filename)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_results_from_query_vector\u001b[39m(query, k\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, matrix_filename\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmatrix\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[39m# Get matrix\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./calculated-components/\u001b[39m\u001b[39m{\u001b[39;00mmatrix_filename\u001b[39m}\u001b[39;00m\u001b[39m.pickle\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m----> 4\u001b[0m         matrix, matrix_norm \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(f)\n\u001b[1;32m      6\u001b[0m     \u001b[39m# Get word set\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m./calculated-components/word_set.pickle\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "search_string = \"compression\"\n",
    "print(\"Matrix results:\")\n",
    "print(get_results_from_query_vector(search_string, k=3, matrix_filename=\"matrix\"))\n",
    "print(\"Matrix-SVD results:\")\n",
    "print(get_results_from_query_vector(search_string, k=3, matrix_filename=\"matrix-svd\"))\n",
    "print(\"Matrix-IDF results:\")\n",
    "print(get_results_from_query_vector(search_string, k=3, matrix_filename=\"matrix-idf\"))\n",
    "print(\"Matrix-SVD-IDF results:\")\n",
    "print(get_results_from_query_vector(search_string, k=3, matrix_filename=\"matrix-svd-idf\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przykładowe zapytanie nr 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix results:\n",
      "[(0.6324555320336759, {'title': 'Layered queueing network', 'pageid': '35685456', 'link': 'https://en.wikipedia.org/wiki?curid=35685456', 'summary': 'In queueing theory, a discipline within the mathematical theory of probability, a layered queueing network (or rendezvous network) is a queueing network model where the service time for each job at each service node is given by the response time of a queueing network (and those service times in turn may also be determined by further nested networks). Resources can be nested and queues form along the nodes of the nesting structure.'}), (0.6047078979069521, {'title': 'Command queue', 'pageid': '5187054', 'link': 'https://en.wikipedia.org/wiki?curid=5187054', 'summary': 'In computer science, a command queue is a queue for enabling the delay of command execution, either in order of priority, on a first-in first-out basis, or in any order that serves the current purpose. Instead of waiting for each command to be executed before sending the next one, the program just puts all the commands in the queue and goes on doing other things while the queue is processed by the operating system.'}), (0.565752381856018, {'title': 'FSCAN', 'pageid': '6334799', 'link': 'https://en.wikipedia.org/wiki?curid=6334799', 'summary': \"FSCAN is a disk scheduling algorithm to determine the motion of the disk's arm and head in servicing read and write requests.\\nIt uses two sub-queues.\"})]\n",
      "Matrix-SVD results:\n",
      "[(0.4839913662937764, {'title': 'Command queue', 'pageid': '5187054', 'link': 'https://en.wikipedia.org/wiki?curid=5187054', 'summary': 'In computer science, a command queue is a queue for enabling the delay of command execution, either in order of priority, on a first-in first-out basis, or in any order that serves the current purpose. Instead of waiting for each command to be executed before sending the next one, the program just puts all the commands in the queue and goes on doing other things while the queue is processed by the operating system.'}), (0.4763982586978574, {'title': 'FSCAN', 'pageid': '6334799', 'link': 'https://en.wikipedia.org/wiki?curid=6334799', 'summary': \"FSCAN is a disk scheduling algorithm to determine the motion of the disk's arm and head in servicing read and write requests.\\nIt uses two sub-queues.\"}), (0.4653636286853846, {'title': 'Layered queueing network', 'pageid': '35685456', 'link': 'https://en.wikipedia.org/wiki?curid=35685456', 'summary': 'In queueing theory, a discipline within the mathematical theory of probability, a layered queueing network (or rendezvous network) is a queueing network model where the service time for each job at each service node is given by the response time of a queueing network (and those service times in turn may also be determined by further nested networks). Resources can be nested and queues form along the nodes of the nesting structure.'})]\n",
      "Matrix-IDF results:\n",
      "[(0.6324555320336759, {'title': 'Layered queueing network', 'pageid': '35685456', 'link': 'https://en.wikipedia.org/wiki?curid=35685456', 'summary': 'In queueing theory, a discipline within the mathematical theory of probability, a layered queueing network (or rendezvous network) is a queueing network model where the service time for each job at each service node is given by the response time of a queueing network (and those service times in turn may also be determined by further nested networks). Resources can be nested and queues form along the nodes of the nesting structure.'}), (0.6047078979069521, {'title': 'Command queue', 'pageid': '5187054', 'link': 'https://en.wikipedia.org/wiki?curid=5187054', 'summary': 'In computer science, a command queue is a queue for enabling the delay of command execution, either in order of priority, on a first-in first-out basis, or in any order that serves the current purpose. Instead of waiting for each command to be executed before sending the next one, the program just puts all the commands in the queue and goes on doing other things while the queue is processed by the operating system.'}), (0.5657523818560181, {'title': 'FSCAN', 'pageid': '6334799', 'link': 'https://en.wikipedia.org/wiki?curid=6334799', 'summary': \"FSCAN is a disk scheduling algorithm to determine the motion of the disk's arm and head in servicing read and write requests.\\nIt uses two sub-queues.\"})]\n",
      "Matrix-SVD-IDF results:\n",
      "[(0.4876839582697722, {'title': 'Command queue', 'pageid': '5187054', 'link': 'https://en.wikipedia.org/wiki?curid=5187054', 'summary': 'In computer science, a command queue is a queue for enabling the delay of command execution, either in order of priority, on a first-in first-out basis, or in any order that serves the current purpose. Instead of waiting for each command to be executed before sending the next one, the program just puts all the commands in the queue and goes on doing other things while the queue is processed by the operating system.'}), (0.4839196269633203, {'title': 'FSCAN', 'pageid': '6334799', 'link': 'https://en.wikipedia.org/wiki?curid=6334799', 'summary': \"FSCAN is a disk scheduling algorithm to determine the motion of the disk's arm and head in servicing read and write requests.\\nIt uses two sub-queues.\"}), (0.4517705418219661, {'title': 'Layered queueing network', 'pageid': '35685456', 'link': 'https://en.wikipedia.org/wiki?curid=35685456', 'summary': 'In queueing theory, a discipline within the mathematical theory of probability, a layered queueing network (or rendezvous network) is a queueing network model where the service time for each job at each service node is given by the response time of a queueing network (and those service times in turn may also be determined by further nested networks). Resources can be nested and queues form along the nodes of the nesting structure.'})]\n"
     ]
    }
   ],
   "source": [
    "search_string = \"queue\"\n",
    "print(\"Matrix results:\")\n",
    "print(get_results_from_query_vector(search_string, k=3, matrix_filename=\"matrix\"))\n",
    "print(\"Matrix-SVD results:\")\n",
    "print(get_results_from_query_vector(search_string, k=3, matrix_filename=\"matrix-svd\"))\n",
    "print(\"Matrix-IDF results:\")\n",
    "print(get_results_from_query_vector(search_string, k=3, matrix_filename=\"matrix-idf\"))\n",
    "print(\"Matrix-SVD-IDF results:\")\n",
    "print(get_results_from_query_vector(search_string, k=3, matrix_filename=\"matrix-svd-idf\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wnioski"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
